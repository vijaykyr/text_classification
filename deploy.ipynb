{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy for Online Prediction\n",
    "\n",
    "Because our model requires some python-level preprocessing to embed our requests, we will take advantage of [AI Platforms Custom Prediction Routines](https://cloud.google.com/ml-engine/docs/tensorflow/custom-prediction-routines) which allows us to execute custom python code in response to every online prediction request. There are 5 steps to creating a custom prediction routine:\n",
    "\n",
    "1. Upload Model Artifacts to GCS\n",
    "2. Implement Predictor interface \n",
    "3. Package the prediction code and dependencies\n",
    "4. Deploy\n",
    "5. Invoke API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Platform 2.0 Dependency\n",
    "\n",
    "This code relies on TF 2.0 which AI Platform online prediction doesn't support yet. Prediction works locally, and the model deploys, but online prediction fails because the AI Platform nodes are running TF 1.13.\n",
    "\n",
    "**We need to re-test this notebook once TF 2.0 is supported for online prediction.** With any luck it will work without any further changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'vijays-sandbox'\n",
    "BUCKET = 'vijays-sandbox-ml'\n",
    "MODEL_PATH = '.'\n",
    "MODEL_NAME = 'headlines'\n",
    "VERSION_NAME = 'v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload Model Artifacts to GCS\n",
    "\n",
    "Here we upload our model to GCS so that AI Platform can access them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./headline_classification_model.h5 [Content-Type=application/octet-stream]...\n",
      "/ [1 files][617.9 KiB/617.9 KiB]                                                \n",
      "Operation completed over 1 objects/617.9 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $MODEL_PATH/headline_classification_model.h5 gs://$BUCKET/headlines/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement Predictor Interface\n",
    "\n",
    "Interface Spec: https://cloud.google.com/ml-engine/docs/tensorflow/custom-prediction-routines#predictor-class\n",
    "\n",
    "This tells AI Platform how to load the model artifacts, and is where we specify our custom prediction code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predictor.py\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "class HeadlinesPredictor(object):\n",
    "    def __init__(self, model):\n",
    "      self.model = model\n",
    "    \n",
    "    def predict(self, instances, **kwargs):\n",
    "        texts = instances\n",
    "        #split sentences into lists of words\n",
    "        texts = [sentence.split() for sentence in texts] \n",
    "        # pad to constant length\n",
    "        texts = [(sentence + MAX_SEQUENCE_LENGTH * ['<PAD>'])[:MAX_SEQUENCE_LENGTH] for sentence in texts] \n",
    "        #embed\n",
    "        embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\")\n",
    "        texts = [embed(sentence) for sentence in texts]\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices(texts).batch(len(texts))\n",
    "        return self.model.predict(dataset)\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, model_dir):\n",
    "        model = tf.keras.models.load_model(os.path.join(model_dir,'headline_classification_model.h5'))\n",
    "    \n",
    "        return cls(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Predictor Class Works Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "techcrunch=[\n",
    "  'Uber shuts down self-driving trucks unit',\n",
    "  'Grover raises €37M Series A to offer latest tech products as a subscription',\n",
    "  'Tech companies can now bid on the Pentagon’s $10B cloud contract'\n",
    "]\n",
    "nytimes=[\n",
    "  '‘Lopping,’ ‘Tips’ and the ‘Z-List’: Bias Lawsuit Explores Harvard’s Admissions',\n",
    "  'A $3B Plan to Turn Hoover Dam into a Giant Battery',\n",
    "  'A MeToo Reckoning in China’s Workplace Amid Wave of Accusations'\n",
    "]\n",
    "github=[\n",
    "  'Show HN: Moon – 3kb JavaScript UI compiler',\n",
    "  'Show HN: Hello, a CLI tool for managing social media',\n",
    "  'Firefox Nightly added support for time-travel debugging'\n",
    "]\n",
    "requests = (techcrunch+nytimes+github)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: In the subsequent cell, if you get a GPU related error, it's likely because the GPU is still bound to the previous notebook. Release it by shutting down that previous notebook session. Then restart the kernel of the current notebook and the error should resolve.\n",
    "\n",
    "<img src='assets/shutdown_session.png' width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0815 19:14:57.433857 139649201391360 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.4379624e-03, 3.5369781e-01, 6.3986421e-01],\n",
       "       [1.9121055e-05, 1.7003361e-02, 9.8297751e-01],\n",
       "       [2.0236766e-03, 1.2312700e-01, 8.7484932e-01],\n",
       "       [1.7740212e-02, 5.8616209e-01, 3.9609775e-01],\n",
       "       [7.3872650e-01, 6.9254123e-02, 1.9201937e-01],\n",
       "       [5.9970737e-02, 4.9645030e-01, 4.4357905e-01],\n",
       "       [9.9892384e-01, 3.9826475e-05, 1.0363412e-03],\n",
       "       [9.2509478e-01, 3.8148116e-03, 7.1090408e-02],\n",
       "       [9.7563666e-01, 6.0183660e-04, 2.3761475e-02]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import predictor\n",
    "\n",
    "predictor = predictor.HeadlinesPredictor.from_path(MODEL_PATH)\n",
    "predictor.predict(requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Package Predictor Class and Dependencies\n",
    "\n",
    "We must package the predictor as a tar.gz source distribution package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='headlines_custom_predict_code',\n",
    "    version='0.1',\n",
    "    scripts=['predictor.py'],\n",
    "    install_requires=[\n",
    "        'tensorflow_hub',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp dist/headlines_custom_predict_code-0.1.tar.gz gs://$BUCKET/headlines/predict_code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy\n",
    "\n",
    "This is similar to how we deploy standard models to AI Platform, with a few extra command line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai-platform models create $MODEL_NAME --regions us-central1 --enable-logging --enable-console-logging\n",
    "\n",
    "#Change --runtime-version to 2.0 when supported\n",
    "!gcloud beta ai-platform versions create $VERSION_NAME \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --runtime-version 1.14 \\\n",
    "  --python-version 3.5 \\\n",
    "  --origin gs://$BUCKET/headlines/model/ \\\n",
    "  --package-uris gs://$BUCKET/headlines/predict_code/headlines_custom_predict_code-0.1.tar.gz \\\n",
    "  --prediction-class predictor.HeadlinesPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Invoke API (this will fail because AI Platform doesn't support 2.0 yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "name = 'projects/{}/models/{}/versions/{}'.format(PROJECT_ID, MODEL_NAME, VERSION_NAME)\n",
    "\n",
    "response = service.projects().predict(\n",
    "    name=name,\n",
    "    body={'instances': requests}\n",
    ").execute()\n",
    "\n",
    "if 'error' in response:\n",
    "    raise RuntimeError(response['error'])\n",
    "else:\n",
    "  print(response['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
