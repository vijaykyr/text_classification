{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Classification using TensorFlow/Keras on Cloud ML Engine </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating datasets for Machine Learning using BigQuery\n",
    "<li> Creating a text classification model using the Estimator API with a Keras model\n",
    "<li> Training on Cloud ML Engine\n",
    "<li> Deploying the model\n",
    "<li> Predicting with model\n",
    "<li> Rerun with pre-trained embedding\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'vijays-sandbox-ml'\n",
    "PROJECT = 'vijays-sandbox'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install tf-nightly-2.0-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20190731\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__) # tf 2.0 nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the titles of articles and figure out whether the article came from the New York Times, TechCrunch or GitHub. \n",
    "\n",
    "We will use [hacker news](https://news.ycombinator.com/) as our data source. It is an aggregator that displays tech related headlines from various  sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset from BigQuery \n",
    "\n",
    "Hacker news headlines are available as a BigQuery public dataset. The [dataset](https://bigquery.cloud.google.com/table/bigquery-public-data:hacker_news.stories?tab=details) contains all headlines from the sites inception in October 2006 until October 2015. \n",
    "\n",
    "Here is a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext google.cloud.bigquery #appears to be pre loaded in ai platform notebook, remove once confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.dumpert.nl/mediabase/6560049/3eb18e...</td>\n",
       "      <td>Calling the NSA: \"I accidentally deleted an e-...</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://blog.liip.ch/archive/2013/10/28/hhvm-an...</td>\n",
       "      <td>Amazing performance with HHVM and PHP with a S...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.gamedev.net/page/resources/_/techni...</td>\n",
       "      <td>A Journey Through the CPU Pipeline</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://jfarcand.wordpress.com/2011/02/25/atmos...</td>\n",
       "      <td>Atmosphere Framework 0.7 released: GWT, Wicket...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://tech.gilt.com/post/90578399884/immutabl...</td>\n",
       "      <td>Immutable Infrastructure with Docker and EC2 [...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://thechangelog.com/post/501053444/episode...</td>\n",
       "      <td>Changelog 0.2.0 - node.js w/Felix Geisendorfer</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://openangelforum.com/2010/09/09/second-bo...</td>\n",
       "      <td>Second Open Angel Forum in Boston Oct 13th--fr...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://bredele.github.io/async</td>\n",
       "      <td>A collection of JavaScript asynchronous patterns</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://www.smashingmagazine.com/2007/08/25/20-...</td>\n",
       "      <td>20 Free and Fresh Icon Sets</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://www.cio.com/article/147801/Study_Finds_...</td>\n",
       "      <td>Study: Only 1 in 5 Workers is \"Engaged\" in The...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://www.dumpert.nl/mediabase/6560049/3eb18e...   \n",
       "1  http://blog.liip.ch/archive/2013/10/28/hhvm-an...   \n",
       "2  http://www.gamedev.net/page/resources/_/techni...   \n",
       "3  http://jfarcand.wordpress.com/2011/02/25/atmos...   \n",
       "4  http://tech.gilt.com/post/90578399884/immutabl...   \n",
       "5  http://thechangelog.com/post/501053444/episode...   \n",
       "6  http://openangelforum.com/2010/09/09/second-bo...   \n",
       "7                     http://bredele.github.io/async   \n",
       "8  http://www.smashingmagazine.com/2007/08/25/20-...   \n",
       "9  http://www.cio.com/article/147801/Study_Finds_...   \n",
       "\n",
       "                                               title  score  \n",
       "0  Calling the NSA: \"I accidentally deleted an e-...    258  \n",
       "1  Amazing performance with HHVM and PHP with a S...     11  \n",
       "2                 A Journey Through the CPU Pipeline     11  \n",
       "3  Atmosphere Framework 0.7 released: GWT, Wicket...     11  \n",
       "4  Immutable Infrastructure with Docker and EC2 [...     11  \n",
       "5     Changelog 0.2.0 - node.js w/Felix Geisendorfer     11  \n",
       "6  Second Open Angel Forum in Boston Oct 13th--fr...     11  \n",
       "7   A collection of JavaScript asynchronous patterns     11  \n",
       "8                        20 Free and Fresh Icon Sets     11  \n",
       "9  Study: Only 1 in 5 Workers is \"Engaged\" in The...     11  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT\n",
    "  url, title, score\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  LENGTH(title) > 10\n",
    "  AND score > 10\n",
    "  AND LENGTH(url) > 0\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some regular expression parsing in BigQuery to get the source of the newspaper article from the URL. For example, if the url is http://mobile.nytimes.com/...., I want to be left with <i>nytimes</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>num_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blogspot</td>\n",
       "      <td>41386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>github</td>\n",
       "      <td>36525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>30891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youtube</td>\n",
       "      <td>30848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nytimes</td>\n",
       "      <td>28787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medium</td>\n",
       "      <td>18422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>google</td>\n",
       "      <td>18235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wordpress</td>\n",
       "      <td>17667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>arstechnica</td>\n",
       "      <td>13749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wired</td>\n",
       "      <td>12841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source  num_articles\n",
       "0     blogspot         41386\n",
       "1       github         36525\n",
       "2   techcrunch         30891\n",
       "3      youtube         30848\n",
       "4      nytimes         28787\n",
       "5       medium         18422\n",
       "6       google         18235\n",
       "7    wordpress         17667\n",
       "8  arstechnica         13749\n",
       "9        wired         12841"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT\n",
    "  ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "  COUNT(title) AS num_articles\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "  AND LENGTH(title) > 10\n",
    "GROUP BY\n",
    "  source\n",
    "ORDER BY num_articles DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good parsing of the URL to get the source, let's put together a dataset of source and titles. This will be our labeled dataset for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>github</td>\n",
       "      <td>erlang  eunit and travis without rebar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>github</td>\n",
       "      <td>node-pngdefry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>github</td>\n",
       "      <td>let s study for google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>github</td>\n",
       "      <td>mcabber-festival   im   speech notifications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>github</td>\n",
       "      <td>an open source pok mon game on ios with locati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                                              title\n",
       "0  github             erlang  eunit and travis without rebar\n",
       "1  github                                      node-pngdefry\n",
       "2  github                             let s study for google\n",
       "3  github       mcabber-festival   im   speech notifications\n",
       "4  github  an open source pok mon game on ios with locati..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT source, LOWER(REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ')) AS title FROM\n",
    "  (SELECT\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "    title\n",
    "  FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "  WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "    AND LENGTH(title) > 10\n",
    "  )\n",
    "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
    "\"\"\"\n",
    "\n",
    "df = bq.query(query + \" LIMIT 5\").to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML training, we will need to split our dataset into training and evaluation datasets (and perhaps an independent test dataset if we are going to do model or feature selection based on the evaluation dataset).  \n",
    "\n",
    "A simple, repeatable way to do this is to use the hash of a well-distributed column in our data (See https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = bq.query(query + \" AND MOD(ABS(FARM_FINGERPRINT(title)),4) > 0\").to_dataframe()\n",
    "evaldf  = bq.query(query + \" AND MOD(ABS(FARM_FINGERPRINT(title)),4) = 0\").to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see that roughly 75% of the data is used for training, and 25% for evaluation. \n",
    "\n",
    "We can also see that within each dataset, the classes are roughly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github        27445\n",
       "techcrunch    23131\n",
       "nytimes       21586\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github        9080\n",
       "techcrunch    7760\n",
       "nytimes       7201\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaldf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will save our data, which is currently in-memory, to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "DATADIR='data/txtcls'\n",
    "shutil.rmtree(DATADIR, ignore_errors=True)\n",
    "os.makedirs(DATADIR)\n",
    "traindf.to_csv( os.path.join(DATADIR,'train.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')\n",
    "evaldf.to_csv( os.path.join(DATADIR,'eval.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "github\tthis guy just found out how to bypass adblocker\n",
      "github\tshow hn  dodo   command line task management for developers\n",
      "github\tshow hn  webservicemock   mock out external calls for local development\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/txtcls/train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  24041 data/txtcls/eval.tsv\n",
      "  72162 data/txtcls/train.tsv\n",
      "  96203 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/txtcls/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras Code\n",
    "\n",
    "Please explore the code in this <a href=\"txtclsmodel/trainer\">directory</a>: `model.py` contains the TensorFlow model and `task.py` parses command line arguments and launches off the training job.\n",
    "\n",
    "In particular look for the following:\n",
    "\n",
    "1. [tf.keras.preprocessing.text.Tokenizer.fit_on_texts()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts) to generate a mapping from our word vocabulary to integers\n",
    "2. [tf.keras.preprocessing.text.Tokenizer.texts_to_sequences()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences) to encode our sentences into a sequence of their respective word-integers\n",
    "3. [tf.keras.preprocessing.sequence.pad_sequences()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) to pad all sequences to be the same length\n",
    "\n",
    "The embedding layer in the keras model takes care of one-hot encoding these integers and learning a dense emedding represetation from them. \n",
    "\n",
    "Finally we pass the embedded text representation through a CNN model pictured below\n",
    "\n",
    "<img src=assets/txtcls_model.png  width=25%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Locally (optional step)\n",
    "Let's make sure the code compiles by running locally for a fraction of an epoch.\n",
    "This may not work if you don't have all the packages installed locally for gcloud (such as in Colab).\n",
    "This is an optional step; move on to training on the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-12 19:29:24.197161: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2019-08-12 19:29:24.473277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:24.473590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-08-12 19:29:24.473878: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-08-12 19:29:24.475292: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-08-12 19:29:24.476478: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-08-12 19:29:24.476760: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-08-12 19:29:24.478213: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-08-12 19:29:24.479375: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-08-12 19:29:24.482707: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-08-12 19:29:24.482823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:24.483124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:24.483380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2019-08-12 19:29:24.483768: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-08-12 19:29:24.575220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:24.575658: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ea1d2a1890 executing computations on platform CUDA. Devices:\n",
      "2019-08-12 19:29:24.575688: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2019-08-12 19:29:24.578723: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "2019-08-12 19:29:24.578976: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ea1d312820 executing computations on platform Host. Devices:\n",
      "2019-08-12 19:29:24.578998: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-08-12 19:29:24.579154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:24.579492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-08-12 19:29:24.579568: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-08-12 19:29:24.579589: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-08-12 19:29:24.579602: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-08-12 19:29:24.579617: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-08-12 19:29:24.579629: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-08-12 19:29:24.579642: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-08-12 19:29:24.579656: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-08-12 19:29:24.579732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:24.580088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:24.580428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2019-08-12 19:29:24.580525: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-08-12 19:29:24.581391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-08-12 19:29:24.581414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2019-08-12 19:29:24.581422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2019-08-12 19:29:24.581683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:24.582007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:24.582327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15190 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "2019-08-12 19:29:25.519419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:25.519688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-08-12 19:29:25.519767: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-08-12 19:29:25.519790: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-08-12 19:29:25.519804: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-08-12 19:29:25.519819: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-08-12 19:29:25.519833: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-08-12 19:29:25.519846: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-08-12 19:29:25.519862: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-08-12 19:29:25.519940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:25.520235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:25.520452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2019-08-12 19:29:25.520509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-08-12 19:29:25.520519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2019-08-12 19:29:25.520526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2019-08-12 19:29:25.520680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:25.520943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:25.521147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15190 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "2019-08-12 19:29:26.162790: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0812 19:29:26.243233 139865038079424 deprecation.py:323] From /home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0812 19:29:26.900613 139865038079424 deprecation.py:323] From /home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2019-08-12 19:29:27.667641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:27.667897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-08-12 19:29:27.667963: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-08-12 19:29:27.667980: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-08-12 19:29:27.667990: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-08-12 19:29:27.667999: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-08-12 19:29:27.668009: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-08-12 19:29:27.668018: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-08-12 19:29:27.668029: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-08-12 19:29:27.668080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:27.668276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:27.668475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2019-08-12 19:29:27.668512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-08-12 19:29:27.668521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2019-08-12 19:29:27.668526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2019-08-12 19:29:27.668635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:27.668833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:27.669011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15190 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "2019-08-12 19:29:30.095867: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-08-12 19:29:30.314671: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-08-12 19:29:32.110592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:32.110922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-08-12 19:29:32.111027: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-08-12 19:29:32.111049: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-08-12 19:29:32.111063: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-08-12 19:29:32.111077: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-08-12 19:29:32.111089: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-08-12 19:29:32.111105: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-08-12 19:29:32.111120: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-08-12 19:29:32.111196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:32.111428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:32.111604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2019-08-12 19:29:32.111648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-08-12 19:29:32.111660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2019-08-12 19:29:32.111667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2019-08-12 19:29:32.111803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:32.112027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:32.112229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15190 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "W0812 19:29:32.112396 139865038079424 deprecation.py:323] From /home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0812 19:29:32.716109 139865038079424 deprecation.py:323] From /home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "2019-08-12 19:29:32.716764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:32.716992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-08-12 19:29:32.717063: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-08-12 19:29:32.717080: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-08-12 19:29:32.717088: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-08-12 19:29:32.717097: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-08-12 19:29:32.717104: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-08-12 19:29:32.717112: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-08-12 19:29:32.717121: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-08-12 19:29:32.717190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:32.717399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:32.717565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2019-08-12 19:29:32.717600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-08-12 19:29:32.717608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2019-08-12 19:29:32.717613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2019-08-12 19:29:32.717733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:32.717941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-08-12 19:29:32.718128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15190 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## Make sure we have the latest version of Google Cloud Storage package\n",
    "#pip install --upgrade google-cloud-storage\n",
    "rm -rf txtcls_trained\n",
    "gcloud ai-platform local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/txtclsmodel/trainer \\\n",
    "   -- \\\n",
    "   --output_dir=${PWD}/txtcls_trained \\\n",
    "   --train_data_path=${PWD}/data/txtcls/train.tsv \\\n",
    "   --eval_data_path=${PWD}/data/txtcls/eval.tsv \\\n",
    "   --num_epochs=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the Cloud\n",
    "\n",
    "Let's first copy our training data to the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://data/txtcls/eval.tsv [Content-Type=text/tab-separated-values]...\n",
      "Copying file://data/txtcls/train.tsv [Content-Type=text/tab-separated-values]...\n",
      "-\n",
      "Operation completed over 2 objects/5.4 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cp data/txtcls/*.tsv gs://${BUCKET}/txtcls/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  #single P100 GPU\n",
    "  masterType: standard_p100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workaround to train with TF 2.0\n",
    "Note that `--runtime-version` is omitted. This is because TF 2.0 is not yet a supported AI Platform Training runtime. To work around this we instead specify it as a required package in our `setup.py` as instructed [here](https://cloud.google.com/ml-engine/docs/versioning#custom-tf-versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: txtcls_190812_193034\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/checkpoint#1565632742233918...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/eval/#1565632439678735...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/#1565632441822088...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/events.out.tfevents.1565632373.cmle-training-2274830552233938779#1565632756586159...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/eval/events.out.tfevents.1565632439.cmle-training-2274830552233938779#1565632748238860...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632508/variables/variables.data-00001-of-00002#1565632516135200...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632571/saved_model.pb#1565632578299604...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/#1565632442155061...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632571/variables/variables.data-00000-of-00002#1565632578644582...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632508/variables/variables.index#1565632516343646...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632508/saved_model.pb#1565632515622791...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632508/#1565632515393783...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632508/variables/#1565632515782217...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632571/variables/#1565632578452985...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632571/variables/variables.data-00001-of-00002#1565632578809747...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632571/variables/variables.index#1565632578976712...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632571/#1565632578134728...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632636/#1565632644890363...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632636/saved_model.pb#1565632645048103...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632508/variables/variables.data-00000-of-00002#1565632515980752...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632636/variables/#1565632645188053...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632636/variables/variables.data-00000-of-00002#1565632645333492...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632636/variables/variables.data-00001-of-00002#1565632645521216...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632636/variables/variables.index#1565632645758024...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632701/#1565632707988294...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632701/saved_model.pb#1565632708222036...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632701/variables/#1565632708414721...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632701/variables/variables.data-00000-of-00002#1565632708563966...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632701/variables/variables.data-00001-of-00002#1565632708741454...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632701/variables/variables.index#1565632708899316...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632748/#1565632754050151...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632748/saved_model.pb#1565632754233076...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632748/variables/variables.data-00000-of-00002#1565632754602848...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632748/variables/variables.data-00001-of-00002#1565632754837350...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632748/variables/#1565632754424422...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/export/exporter/1565632748/variables/variables.index#1565632755013859...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/graph.pbtxt#1565632379333903...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/keras/#1565632365566316...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/keras/checkpoint#1565632370403851...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/keras/keras_model.ckpt.data-00000-of-00002#1565632369353757...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/keras/keras_model.ckpt.data-00001-of-00002#1565632369080677...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/keras/keras_model.ckpt.index#1565632369634302...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-1000.data-00000-of-00002#1565632501230041...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-1000.data-00001-of-00002#1565632500936240...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-1000.index#1565632501531705...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-1000.meta#1565632503278569...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-1500.data-00000-of-00002#1565632565214592...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-1500.data-00001-of-00002#1565632564936032...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-1500.index#1565632565504957...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-1500.meta#1565632567086663...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-2000.data-00000-of-00002#1565632627846017...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-2000.data-00001-of-00002#1565632627554140...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-2000.meta#1565632629722618...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-2000.index#1565632628082396...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-2500.data-00000-of-00002#1565632694897300...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-2500.index#1565632695175200...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-2500.meta#1565632697216771...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-2500.data-00001-of-00002#1565632694656152...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-2819.data-00000-of-00002#1565632741172906...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-2819.data-00001-of-00002#1565632740881365...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-2819.index#1565632741478587...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/model.ckpt-2819.meta#1565632743554594...\n",
      "Removing gs://vijays-sandbox-ml/txtcls/trained_fromscratch/packages/db187bbd5f1c4efd74caedaa36d3b83faa8eab9718b7e3ca6770ec9d61472fd1/text_classification-1.0.tar.gz#1565632236322905...\n",
      "/ [63/63 objects] 100% Done                                                     \n",
      "Operation completed over 63 objects.                                             \n",
      "Job [txtcls_190812_193034] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe txtcls_190812_193034\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs txtcls_190812_193034\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/txtcls/trained_fromscratch\n",
    "JOBNAME=txtcls_$(date -u +%y%m%d_%H%M%S)\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    " --region=$REGION \\\n",
    " --module-name=trainer.task \\\n",
    " --package-path=${PWD}/txtclsmodel/trainer \\\n",
    " --job-dir=$OUTDIR \\\n",
    " --config config.yaml \\\n",
    " --python-version 3.5 \\\n",
    " -- \\\n",
    " --output_dir=$OUTDIR \\\n",
    " --train_data_path=gs://${BUCKET}/txtcls/train.tsv \\\n",
    " --eval_data_path=gs://${BUCKET}/txtcls/eval.tsv \\\n",
    " --num_epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the job name appropriately. View the job in the console, and wait until the job is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor training with TensorBoard\n",
    "\n",
    "If Tensorboard appears blank, try refreshing after a few minutes.\n",
    "If you removed the directory that Tensorboard was monitoring, stop and restart Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGROK_DIR = './assets/ngrok'\n",
    "OUTDIR='gs://{}/txtcls'.format(BUCKET)\n",
    "\n",
    "get_ipython().system_raw(\n",
    "    \"tensorboard --logdir {} --host 0.0.0.0 --port 6006 &\"\n",
    "    .format(OUTDIR)\n",
    ")\n",
    "\n",
    "get_ipython().system_raw(\"{} http 6006 &\".format(NGROK_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://0e72b97b.ngrok.io\n"
     ]
    }
   ],
   "source": [
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: (10040): No such process\n"
     ]
    }
   ],
   "source": [
    "# this will kill the processes for Tensorboard\n",
    "!ps aux | grep tensorboard | awk '{print $2}' | xargs kill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: (10046): No such process\n"
     ]
    }
   ],
   "source": [
    "# this will kill the processes for ngrok\n",
    "!ps aux | grep ngrok | awk '{print $2}' | xargs kill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "What accuracy did you get? You should see around 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy trained model \n",
    "\n",
    "Once your training completes you will see your exported models in the output directory specified in Google Cloud Storage. \n",
    "\n",
    "You should see one model for each training checkpoint (default is every 1000 steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/txtcls/trained_fromscratch/export/exporter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take the last export and deploy it as a REST API using Google Cloud Machine Learning Engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"txtcls\"\n",
    "MODEL_VERSION=\"v1_fromscratch\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/txtcls/trained_fromscratch/export/exporter/ | tail -1)\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME} --quiet\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=$TFVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions\n",
    "\n",
    "Here are some actual hacker news headlines gathered from July 2018. These titles were not part of the training or evaluation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techcrunch=[\n",
    "  'Uber shuts down self-driving trucks unit',\n",
    "  'Grover raises €37M Series A to offer latest tech products as a subscription',\n",
    "  'Tech companies can now bid on the Pentagon’s $10B cloud contract'\n",
    "]\n",
    "nytimes=[\n",
    "  '‘Lopping,’ ‘Tips’ and the ‘Z-List’: Bias Lawsuit Explores Harvard’s Admissions',\n",
    "  'A $3B Plan to Turn Hoover Dam into a Giant Battery',\n",
    "  'A MeToo Reckoning in China’s Workplace Amid Wave of Accusations'\n",
    "]\n",
    "github=[\n",
    "  'Show HN: Moon – 3kb JavaScript UI compiler',\n",
    "  'Show HN: Hello, a CLI tool for managing social media',\n",
    "  'Firefox Nightly added support for time-travel debugging'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our serving input function expects the already tokenized representations of the headlines, so we do that pre-processing in the code before calling the REST API.\n",
    "\n",
    "Note: Ideally we would do these transformation in the tensorflow graph directly instead of relying on separate client pre-processing code (see: [training-serving skew](https://developers.google.com/machine-learning/guides/rules-of-ml/#training_serving_skew)), howevever the pre-processing functions we're using are python functions so cannot be embedded in a tensorflow graph. \n",
    "\n",
    "See the <a href=\"text_classification_native.ipynb\">text_classification_native</a> notebook for a solution to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "requests = techcrunch+nytimes+github\n",
    "\n",
    "# Tokenize and pad sentences using same mapping used in the deployed model\n",
    "tokenizer = pickle.load( open( \"txtclsmodel/tokenizer.pickled\", \"rb\" ) )\n",
    "\n",
    "requests_tokenized = tokenizer.texts_to_sequences(requests)\n",
    "requests_tokenized = sequence.pad_sequences(requests_tokenized,maxlen=50)\n",
    "\n",
    "# JSON format the requests\n",
    "request_data = {'instances':requests_tokenized.tolist()}\n",
    "\n",
    "# Authenticate and call CMLE prediction API \n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json')\n",
    "\n",
    "parent = 'projects/%s/models/%s' % (PROJECT, 'txtcls') #version is not specified so uses default\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "\n",
    "# Format and print response\n",
    "for i in range(len(requests)):\n",
    "  print('\\n{}'.format(requests[i]))\n",
    "  print(' github    : {}'.format(response['predictions'][i]['dense_1'][0]))\n",
    "  print(' nytimes   : {}'.format(response['predictions'][i]['dense_1'][1]))\n",
    "  print(' techcrunch: {}'.format(response['predictions'][i]['dense_1'][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of your predictions were correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun with Pre-trained Embedding\n",
    "\n",
    "In the previous model we trained our word embedding from scratch. Often times we get better performance and/or converge faster by leveraging a pre-trained embedding. This is a similar concept to transfer learning during image classification.\n",
    "\n",
    "We will use the popular GloVe embedding which is trained on Wikipedia as well as various news sources like the New York Times.\n",
    "\n",
    "You can read more about Glove at the project homepage: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "You can download the embedding files directly from the stanford.edu site, but we've rehosted it in a GCS bucket for faster download speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/09_sequence/text_classification/glove.6B.200d.txt [Content-Type=text/plain]...\n",
      "- [1 files][661.3 MiB/661.3 MiB]      0.0 B/s                                   \n",
      "Operation completed over 1 objects/661.3 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://cloud-training-demos/courses/machine_learning/deepdive/09_sequence/text_classification/glove.6B.200d.txt gs://$BUCKET/txtcls/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the embedding is downloaded re-run your cloud training job with the added command line argument: \n",
    "\n",
    "` --embedding_path=gs://${BUCKET}/txtcls/glove.6B.200d.txt`\n",
    "\n",
    "Be sure to change your OUTDIR so it doesn't overwrite the previous model.\n",
    "\n",
    "While the final accuracy may not change significantly, you should notice the model is able to converge to it much more quickly because it no longer has to learn an embedding from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- This implementation is based on code from: https://github.com/google/eng-edu/tree/master/ml/guides/text_classification.\n",
    "- See the full text classification tutorial at: https://developers.google.com/machine-learning/guides/text-classification/\n",
    "-\n",
    "\n",
    "## Next step\n",
    "Client-side tokenizing in Python is hugely problematic. See <a href=\"text_classification_native.ipynb\">Text classification with native serving</a> for how to carry out the preprocessing in the serving function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
