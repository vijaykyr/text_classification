{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Classification using TensorFlow/Keras on Cloud ML Engine </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating datasets for Machine Learning using BigQuery\n",
    "<li> Using TF Hub for transfer learning\n",
    "<li> Creating a sentence level text classification model using Keras\n",
    "<li> Creating a word lelvel text classification model using Keras\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'vijays-sandbox-ml'\n",
    "PROJECT = 'vijays-sandbox'\n",
    "REGION = 'us-central1'\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install tf-nightly-2.0-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__) # tf 2.0 nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the titles of articles and figure out whether the article came from the New York Times, TechCrunch or GitHub. \n",
    "\n",
    "We will use [hacker news](https://news.ycombinator.com/) as our data source. It is an aggregator that displays tech related headlines from various  sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset from BigQuery \n",
    "\n",
    "Hacker news headlines are available as a BigQuery public dataset. The [dataset](https://bigquery.cloud.google.com/table/bigquery-public-data:hacker_news.stories?tab=details) contains all headlines from the sites inception in October 2006 until October 2015. \n",
    "\n",
    "Here is a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext google.cloud.bigquery #appears to be pre loaded in ai platform notebook, remove once confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT\n",
    "  url, title, score\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  LENGTH(title) > 10\n",
    "  AND score > 10\n",
    "  AND LENGTH(url) > 0\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some regular expression parsing in BigQuery to get the source of the newspaper article from the URL. For example, if the url is http://mobile.nytimes.com/...., I want to be left with <i>nytimes</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT\n",
    "  ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "  COUNT(title) AS num_articles\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "  AND LENGTH(title) > 10\n",
    "GROUP BY\n",
    "  source\n",
    "ORDER BY num_articles DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good parsing of the URL to get the source, let's put together a dataset of source and titles. This will be our labeled dataset for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT source, LOWER(REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ')) AS title FROM\n",
    "  (SELECT\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "    title\n",
    "  FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "  WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "    AND LENGTH(title) > 10\n",
    "  )\n",
    "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
    "\"\"\"\n",
    "\n",
    "df = bq.query(query + \" LIMIT 5\").to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML training, we will need to split our dataset into training and evaluation datasets (and perhaps an independent test dataset if we are going to do model or feature selection based on the evaluation dataset).  \n",
    "\n",
    "A simple, repeatable way to do this is to use the hash of a well-distributed column in our data (See https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = bq.query(query + \" AND MOD(ABS(FARM_FINGERPRINT(title)),4) > 0\").to_dataframe()\n",
    "evaldf  = bq.query(query + \" AND MOD(ABS(FARM_FINGERPRINT(title)),4) = 0\").to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see that roughly 75% of the data is used for training, and 25% for evaluation. \n",
    "\n",
    "We can also see that within each dataset, the classes are roughly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will save our data, which is currently in-memory, to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "DATADIR='data/txtcls'\n",
    "shutil.rmtree(DATADIR, ignore_errors=True)\n",
    "os.makedirs(DATADIR)\n",
    "traindf.to_csv( os.path.join(DATADIR,'train.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')\n",
    "evaldf.to_csv( os.path.join(DATADIR,'eval.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 data/txtcls/train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l data/txtcls/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Level Model with DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "\n",
    "CLASSES = {'github': 0, 'nytimes': 1, 'techcrunch': 2}  # label-to-int mapping\n",
    "TOP_K = 20000  # Limit on the number vocabulary size used for tokenization\n",
    "MAX_SEQUENCE_LENGTH = 50  # Sentences will be truncated/padded to this length\n",
    "\n",
    "\"\"\"\n",
    "Helper function to download data from Google Cloud Storage\n",
    "  # Arguments:\n",
    "      source: string, the GCS URL to download from (e.g. 'gs://bucket/file.csv')\n",
    "      destination: string, the filename to save as on local disk. MUST be filename\n",
    "      ONLY, doesn't support folders. (e.g. 'file.csv', NOT 'folder/file.csv')\n",
    "  # Returns: nothing, downloads file to local disk\n",
    "\"\"\"\n",
    "def download_from_gcs(source, destination):\n",
    "    search = re.search('gs://(.*?)/(.*)', source)\n",
    "    bucket_name = search.group(1)\n",
    "    blob_name = search.group(2)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    bucket.blob(blob_name).download_to_filename(destination)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parses raw tsv containing hacker news headlines and returns (sentence, integer label) pairs\n",
    "  # Arguments:\n",
    "      train_data_path: string, path to tsv containing training data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "      eval_data_path: string, path to tsv containing eval data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "  # Returns:\n",
    "      ((train_sentences, train_labels), (test_sentences, test_labels)):  sentences\n",
    "        are lists of strings, labels are numpy integer arrays\n",
    "\"\"\"\n",
    "def load_hacker_news_data(train_data_path, eval_data_path):\n",
    "    if train_data_path.startswith('gs://'):\n",
    "        download_from_gcs(train_data_path, destination='train.csv')\n",
    "        train_data_path = 'train.csv'\n",
    "    if eval_data_path.startswith('gs://'):\n",
    "        download_from_gcs(eval_data_path, destination='eval.csv')\n",
    "        eval_data_path = 'eval.csv'\n",
    "\n",
    "    # Parse CSV using pandas\n",
    "    column_names = ('label', 'text')\n",
    "    df_train = pd.read_csv(train_data_path, names=column_names, sep='\\t')\n",
    "    df_eval = pd.read_csv(eval_data_path, names=column_names, sep='\\t')\n",
    "\n",
    "    return ((list(df_train['text']), np.array(df_train['label'].map(CLASSES))),\n",
    "            (list(df_eval['text']), np.array(df_eval['label'].map(CLASSES))))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create tf.estimator compatible input function\n",
    "  # Arguments:\n",
    "      texts: [strings], list of sentences\n",
    "      labels: numpy int vector, integer labels for sentences\n",
    "      batch_size: int, number of records to use for each train batch\n",
    "      mode: tf.estimator.ModeKeys.TRAIN or tf.estimator.ModeKeys.EVAL \n",
    "  # Returns:\n",
    "      tf.data.dataset, produces feature and label\n",
    "        tensors one batch at a time\n",
    "\"\"\"\n",
    "def input_fn(texts, labels, batch_size, mode):\n",
    "    # Transform text to sequence of integers\n",
    "    labels = tf.one_hot(labels,len(CLASSES)) #precision and recall metrics require one hot labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return dataset.batch(batch_size)\n",
    "    else: \n",
    "        return dataset.shuffle(50000).batch(batch_size)\n",
    "\n",
    "\"\"\"\n",
    "Builds a CNN model using keras and converts to tf.estimator.Estimator\n",
    "  # Arguments\n",
    "      model_dir: string, file path where training files will be written\n",
    "      config: tf.estimator.RunConfig, specifies properties of tf Estimator\n",
    "      filters: int, output dimension of the layers.\n",
    "      kernel_size: int, length of the convolution window.\n",
    "      embedding_dim: int, dimension of the embedding vectors.\n",
    "      dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "      pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "      embedding_path: string , file location of pre-trained embedding (if used)\n",
    "        defaults to None which will cause the model to train embedding from scratch\n",
    "      word_index: dictionary, mapping of vocabulary to integers. used only if\n",
    "        pre-trained embedding is provided\n",
    "\n",
    "    # Returns\n",
    "        A keras model\n",
    "\"\"\"\n",
    "def keras_model(learning_rate):\n",
    "    # Create model instance.\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer\n",
    "    hub_layer = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\", \n",
    "        output_shape=[128], \n",
    "        input_shape=[], \n",
    "        dtype=tf.string\n",
    "    )\n",
    "    model.add(hub_layer)\n",
    "    model.add(Dense(500,activation='relu'))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'train_data_path':'./data/txtcls/train.tsv',\n",
    "    'eval_data_path':'./data/txtcls/eval.tsv',\n",
    "    'batch_size':128,\n",
    "    'learning_rate':.001\n",
    "}\n",
    "\n",
    "# Load Data\n",
    "((train_texts, train_labels), (test_texts, test_labels)) = load_hacker_news_data(\n",
    "    hparams['train_data_path'], hparams['eval_data_path'])\n",
    "\n",
    "model = keras_model(learning_rate=hparams['learning_rate'])\n",
    "\n",
    "train_dataset = input_fn(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    hparams['batch_size'],\n",
    "    mode=tf.estimator.ModeKeys.TRAIN\n",
    ")\n",
    "eval_dataset = input_fn(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    hparams['batch_size'],\n",
    "    mode=tf.estimator.ModeKeys.EVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "564/564 [==============================] - 7s 12ms/step - loss: 0.5376 - accuracy: 0.7716 - precision_2: 0.8022 - recall_2: 0.7341 - val_loss: 0.5059 - val_accuracy: 0.7823 - val_precision_2: 0.8018 - val_recall_2: 0.7591\n",
      "Epoch 2/5\n",
      "564/564 [==============================] - 5s 9ms/step - loss: 0.4562 - accuracy: 0.8098 - precision_2: 0.8293 - recall_2: 0.7867 - val_loss: 0.4819 - val_accuracy: 0.7957 - val_precision_2: 0.8139 - val_recall_2: 0.7752\n",
      "Epoch 3/5\n",
      "564/564 [==============================] - 5s 9ms/step - loss: 0.4166 - accuracy: 0.8292 - precision_2: 0.8458 - recall_2: 0.8096 - val_loss: 0.4781 - val_accuracy: 0.8003 - val_precision_2: 0.8162 - val_recall_2: 0.7830\n",
      "Epoch 4/5\n",
      "564/564 [==============================] - 5s 9ms/step - loss: 0.3841 - accuracy: 0.8447 - precision_2: 0.8586 - recall_2: 0.8281 - val_loss: 0.4847 - val_accuracy: 0.7988 - val_precision_2: 0.8135 - val_recall_2: 0.7833\n",
      "Epoch 5/5\n",
      "564/564 [==============================] - 6s 11ms/step - loss: 0.3555 - accuracy: 0.8573 - precision_2: 0.8693 - recall_2: 0.8441 - val_loss: 0.4975 - val_accuracy: 0.7958 - val_precision_2: 0.8104 - val_recall_2: 0.7821\n",
      "CPU times: user 40.3 s, sys: 4.47 s, total: 44.8 s\n",
      "Wall time: 28.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f088e61b898>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(SEED)\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=eval_dataset,\n",
    "    validation_steps=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 80% accuracy. Not bad, however this method was using sentence level embeddigns and ignoring the ordering of words. Would we get better performance if we embedded each word individually then fed them into a sequential model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Level Model with CNN\n",
    "\n",
    "This model takes into account the order of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "\"\"\"\n",
    "Create tf.estimator compatible input function\n",
    "  # Arguments:\n",
    "      texts: [strings], list of sentences\n",
    "      labels: numpy int vector, integer labels for sentences\n",
    "      tokenizer: tf.python.keras.preprocessing.text.Tokenizer\n",
    "        used to convert sentences to integers\n",
    "      batch_size: int, number of records to use for each train batch\n",
    "      mode: tf.estimator.ModeKeys.TRAIN or tf.estimator.ModeKeys.EVAL \n",
    "  # Returns:\n",
    "      tf.data.dataset, produces feature and label\n",
    "        tensors one batch at a time\n",
    "\"\"\"\n",
    "def input_fn(texts, labels, batch_size, mode):\n",
    "    #precision and recall metrics require one hot labels\n",
    "    labels = tf.one_hot(labels,len(CLASSES)) \n",
    "    #split sentences into lists of words\n",
    "    texts = [sentence.split() for sentence in texts] \n",
    "    # pad to constant length\n",
    "    texts = [(sentence + MAX_SEQUENCE_LENGTH * ['<PAD>'])[:MAX_SEQUENCE_LENGTH] for sentence in texts] \n",
    "    #embed\n",
    "    embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\")\n",
    "    texts = [embed(sentence) for sentence in texts]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return dataset.batch(batch_size)\n",
    "    else: \n",
    "        return dataset.shuffle(50000).batch(batch_size)\n",
    "\n",
    "\"\"\"\n",
    "Builds a CNN model using keras \n",
    "  # Arguments\n",
    "      model_dir: string, file path where training files will be written\n",
    "      config: tf.estimator.RunConfig, specifies properties of tf Estimator\n",
    "      filters: int, output dimension of the layers.\n",
    "      kernel_size: int, length of the convolution window.\n",
    "      embedding_dim: int, dimension of the embedding vectors.\n",
    "      dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "      pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "\n",
    "\n",
    "    # Returns\n",
    "        A tf.estimator.Estimator \n",
    "\"\"\"\n",
    "def keras_model(learning_rate, filters=64, dropout_rate=0.2, kernel_size=3, pool_size=3):\n",
    "    # Create model instance.\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(Dropout(input_shape=(MAX_SEQUENCE_LENGTH,128),rate=dropout_rate))\n",
    "    model.add(Conv1D(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        activation='relu',\n",
    "        bias_initializer='random_uniform',\n",
    "        padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(\n",
    "        filters=filters * 2,\n",
    "        kernel_size=kernel_size,\n",
    "        activation='relu',\n",
    "        bias_initializer='random_uniform',\n",
    "        padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this will take several minutes because now we are doing a lot of pre-processing in the input function. In particular we are embedding each individual word and storing the vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 4s, sys: 45.7 s, total: 5min 50s\n",
      "Wall time: 4min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = input_fn(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    hparams['batch_size'],\n",
    "    mode=tf.estimator.ModeKeys.TRAIN\n",
    ")\n",
    "eval_dataset = input_fn(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    hparams['batch_size'],\n",
    "    mode=tf.estimator.ModeKeys.EVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "564/564 [==============================] - 9s 17ms/step - loss: 0.6690 - accuracy: 0.7032 - precision_4: 0.7619 - recall_4: 0.6068 - val_loss: 0.5172 - val_accuracy: 0.7905 - val_precision_4: 0.8194 - val_recall_4: 0.7484\n",
      "Epoch 2/5\n",
      "564/564 [==============================] - 7s 13ms/step - loss: 0.4999 - accuracy: 0.7949 - precision_4: 0.8197 - recall_4: 0.7635 - val_loss: 0.4679 - val_accuracy: 0.8077 - val_precision_4: 0.8328 - val_recall_4: 0.7792\n",
      "Epoch 3/5\n",
      "564/564 [==============================] - 7s 13ms/step - loss: 0.4642 - accuracy: 0.8088 - precision_4: 0.8305 - recall_4: 0.7829 - val_loss: 0.4449 - val_accuracy: 0.8157 - val_precision_4: 0.8392 - val_recall_4: 0.7907\n",
      "Epoch 4/5\n",
      "564/564 [==============================] - 8s 13ms/step - loss: 0.4392 - accuracy: 0.8207 - precision_4: 0.8415 - recall_4: 0.7960 - val_loss: 0.4285 - val_accuracy: 0.8236 - val_precision_4: 0.8458 - val_recall_4: 0.8000\n",
      "Epoch 5/5\n",
      "564/564 [==============================] - 8s 14ms/step - loss: 0.4175 - accuracy: 0.8319 - precision_4: 0.8503 - recall_4: 0.8086 - val_loss: 0.4204 - val_accuracy: 0.8274 - val_precision_4: 0.8466 - val_recall_4: 0.8049\n",
      "CPU times: user 58.9 s, sys: 13.2 s, total: 1min 12s\n",
      "Wall time: 58.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f07b02e3780>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = keras_model(learning_rate=hparams['learning_rate'])\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=eval_dataset,\n",
    "    validation_steps=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy improved to 83%! Looks like paying attention to word order does help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('headline_classification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy trained model \n",
    "\n",
    "See [deploy.ipynb](deploy.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
