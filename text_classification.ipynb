{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Classification using TensorFlow/Keras on Cloud ML Engine </h1>\n",
    "\n",
    "We will look at the titles of articles and figure out whether the article came from the New York Times, TechCrunch or GitHub. \n",
    "\n",
    "We will use [hacker news](https://news.ycombinator.com/) as our data source. It is an aggregator that displays tech related headlines from various  sources.\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating datasets for Machine Learning using BigQuery\n",
    "<li> Using TF Hub for transfer learning\n",
    "<li> Creating a sentence level text classification model using Keras\n",
    "<li> Creating a word lelvel text classification model using Keras\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'vijays-sandbox-ml'\n",
    "PROJECT = 'vijays-sandbox'\n",
    "REGION = 'us-central1'\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Strongly Recommended\n",
    "\n",
    "This entire notebook will run in under 10 minutes using a V100 GPU, but will take about 3 hours on CPU\n",
    "\n",
    "You can add a GPU to your AI Platform Notebook instance following [these instructions](https://cloud.google.com/ml-engine/docs/notebooks/manage-hardware-accelerators).\n",
    "\n",
    "After adding the subsequent cell should print \"GPU Enabled: True\". To manage costs, you can remove the GPU after completing the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "GPU Enabled: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__) # tested with tf 2.0.0-beta1\n",
    "print('GPU Enabled: {}'.format(tf.test.is_gpu_available())) # GPU Recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset from BigQuery \n",
    "\n",
    "Hacker news headlines are available as a BigQuery public dataset. The [dataset](https://bigquery.cloud.google.com/table/bigquery-public-data:hacker_news.stories?tab=details) contains all headlines from the sites inception in October 2006 until October 2015. \n",
    "\n",
    "Here is a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://alsop-louie.com/portfolio/portfolio-rev...</td>\n",
       "      <td>Portfolio Review: Justin.tv</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://torrentfreak.com/dutch-isps-ordered-to-...</td>\n",
       "      <td>Dutch ISPs ordered to block The Pirate Bay</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://devstand.com/2012/02/03/impressive-3d-h...</td>\n",
       "      <td>Stunning 3D Examples of HTML5 Artwork</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.erlang-factory.com/conference/SFBay...</td>\n",
       "      <td>2010 SF Bay Area Erlang Factory Programme</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.businessinsider.com/dollar-shave-cl...</td>\n",
       "      <td>Razors, what a DEAL</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://zorter.com/rankers/yc</td>\n",
       "      <td>Show HN: Review Zorter and find the best YC co...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://blog.tippingpointlabs.com/2009/09/12sec...</td>\n",
       "      <td>What Any Startup or VC Can Learn from 12Second...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://www.youtube.com/watch?v=m3giY2eI65w</td>\n",
       "      <td>Impeach Obama for NSA Spying Program?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://thebottomline.cpaaustralia.com.au/</td>\n",
       "      <td>60 Minute Interview with Neil Armstrong</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://codebrief.com/2012/01/the-top-10-javasc...</td>\n",
       "      <td>Top Javascript MVC Frameworks Reviewed</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://alsop-louie.com/portfolio/portfolio-rev...   \n",
       "1  http://torrentfreak.com/dutch-isps-ordered-to-...   \n",
       "2  http://devstand.com/2012/02/03/impressive-3d-h...   \n",
       "3  http://www.erlang-factory.com/conference/SFBay...   \n",
       "4  http://www.businessinsider.com/dollar-shave-cl...   \n",
       "5                       http://zorter.com/rankers/yc   \n",
       "6  http://blog.tippingpointlabs.com/2009/09/12sec...   \n",
       "7         http://www.youtube.com/watch?v=m3giY2eI65w   \n",
       "8          http://thebottomline.cpaaustralia.com.au/   \n",
       "9  http://codebrief.com/2012/01/the-top-10-javasc...   \n",
       "\n",
       "                                               title  score  \n",
       "0                        Portfolio Review: Justin.tv     11  \n",
       "1         Dutch ISPs ordered to block The Pirate Bay     11  \n",
       "2              Stunning 3D Examples of HTML5 Artwork     11  \n",
       "3          2010 SF Bay Area Erlang Factory Programme     11  \n",
       "4                               Razors, what a DEAL      11  \n",
       "5  Show HN: Review Zorter and find the best YC co...     11  \n",
       "6  What Any Startup or VC Can Learn from 12Second...     11  \n",
       "7              Impeach Obama for NSA Spying Program?     11  \n",
       "8            60 Minute Interview with Neil Armstrong     11  \n",
       "9             Top Javascript MVC Frameworks Reviewed     11  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT\n",
    "  url, title, score\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  LENGTH(title) > 10\n",
    "  AND score > 10\n",
    "  AND LENGTH(url) > 0\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some regular expression parsing in BigQuery to get the source of the newspaper article from the URL. For example, if the url is http://mobile.nytimes.com/...., I want to be left with <i>nytimes</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>num_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blogspot</td>\n",
       "      <td>41386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>github</td>\n",
       "      <td>36525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>30891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youtube</td>\n",
       "      <td>30848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nytimes</td>\n",
       "      <td>28787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medium</td>\n",
       "      <td>18422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>google</td>\n",
       "      <td>18235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wordpress</td>\n",
       "      <td>17667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>arstechnica</td>\n",
       "      <td>13749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wired</td>\n",
       "      <td>12841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source  num_articles\n",
       "0     blogspot         41386\n",
       "1       github         36525\n",
       "2   techcrunch         30891\n",
       "3      youtube         30848\n",
       "4      nytimes         28787\n",
       "5       medium         18422\n",
       "6       google         18235\n",
       "7    wordpress         17667\n",
       "8  arstechnica         13749\n",
       "9        wired         12841"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT\n",
    "  ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "  COUNT(title) AS num_articles\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "  AND LENGTH(title) > 10\n",
    "GROUP BY\n",
    "  source\n",
    "ORDER BY num_articles DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good parsing of the URL to get the source, let's put together a dataset of source and titles. This will be our labeled dataset for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>github</td>\n",
       "      <td>php bdd is now  nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>github</td>\n",
       "      <td>mpv video player 0.2 release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>github</td>\n",
       "      <td>show hn  re-thinking the business card with dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>github</td>\n",
       "      <td>update css js from chrome developer tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>github</td>\n",
       "      <td>simple way to start development with flask usi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                                              title\n",
       "0  github                              php bdd is now  nice \n",
       "1  github                       mpv video player 0.2 release\n",
       "2  github  show hn  re-thinking the business card with dr...\n",
       "3  github           update css js from chrome developer tool\n",
       "4  github  simple way to start development with flask usi..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT source, LOWER(REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ')) AS title FROM\n",
    "  (SELECT\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "    title\n",
    "  FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "  WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "    AND LENGTH(title) > 10\n",
    "  )\n",
    "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
    "\"\"\"\n",
    "\n",
    "df = bq.query(query + \" LIMIT 5\").to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML training, we will need to split our dataset into training and evaluation datasets (and perhaps an independent test dataset if we are going to do model or feature selection based on the evaluation dataset).  \n",
    "\n",
    "A simple, repeatable way to do this is to use the hash of a well-distributed column in our data (See https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = bq.query(query + \" AND MOD(ABS(FARM_FINGERPRINT(title)),4) > 0\").to_dataframe()\n",
    "evaldf  = bq.query(query + \" AND MOD(ABS(FARM_FINGERPRINT(title)),4) = 0\").to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see that roughly 75% of the data is used for training, and 25% for evaluation. \n",
    "\n",
    "We can also see that within each dataset, the classes are roughly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github        27445\n",
       "techcrunch    23131\n",
       "nytimes       21586\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github        9080\n",
       "techcrunch    7760\n",
       "nytimes       7201\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaldf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will save our data, which is currently in-memory, to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "DATADIR='data/txtcls'\n",
    "shutil.rmtree(DATADIR, ignore_errors=True)\n",
    "os.makedirs(DATADIR)\n",
    "traindf.to_csv( os.path.join(DATADIR,'train.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')\n",
    "evaldf.to_csv( os.path.join(DATADIR,'eval.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "github\tthis guy just found out how to bypass adblocker\n",
      "github\tshow hn  dodo   command line task management for developers\n",
      "github\tshow hn  webservicemock   mock out external calls for local development\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/txtcls/train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  24041 data/txtcls/eval.tsv\n",
      "  72162 data/txtcls/train.tsv\n",
      "  96203 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/txtcls/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Level Model with DNN\n",
    "\n",
    "Now that we have our dataset, we need to represent our text data numerically. [Tensorflow Hub](https://www.tensorflow.org/hub) makes this super easy. It contains a library of pre-trained text embeddings that we can download and use with a few lines of code. \n",
    "\n",
    "In particular we will use [this](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1) embedding which encodes sentences into 128 dimensional vectors.\n",
    "\n",
    "Once we have the embedded representation we can simply feed it through a DNN for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "\n",
    "CLASSES = {'github': 0, 'nytimes': 1, 'techcrunch': 2}  # label-to-int mapping\n",
    "MAX_SEQUENCE_LENGTH = 50  # Sentences will be truncated/padded to this length\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parses raw tsv containing hacker news headlines and returns (sentence, integer label) pairs\n",
    "  # Arguments:\n",
    "      train_data_path: string, path to tsv containing training data.\n",
    "      eval_data_path: string, path to tsv containing eval data.\n",
    "  # Returns:\n",
    "      ((train_sentences, train_labels), (test_sentences, test_labels)):  sentences\n",
    "        are lists of strings, labels are numpy integer arrays\n",
    "\"\"\"\n",
    "def load_hacker_news_data(train_data_path, eval_data_path):\n",
    "    # Parse CSV using pandas\n",
    "    column_names = ('label', 'text')\n",
    "    df_train = pd.read_csv(train_data_path, names=column_names, sep='\\t')\n",
    "    df_eval = pd.read_csv(eval_data_path, names=column_names, sep='\\t')\n",
    "\n",
    "    return ((list(df_train['text']), np.array(df_train['label'].map(CLASSES))),\n",
    "            (list(df_eval['text']), np.array(df_eval['label'].map(CLASSES))))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create tf.estimator compatible input function\n",
    "  # Arguments:\n",
    "      texts: [strings], list of sentences\n",
    "      labels: numpy int vector, integer labels for sentences\n",
    "      batch_size: int, number of records to use for each train batch\n",
    "      mode: tf.estimator.ModeKeys.TRAIN or tf.estimator.ModeKeys.EVAL \n",
    "  # Returns:\n",
    "      tf.data.dataset, produces feature and label\n",
    "        tensors one batch at a time\n",
    "\"\"\"\n",
    "def input_fn(texts, labels, batch_size, mode):\n",
    "    # Transform text to sequence of integers\n",
    "    labels = tf.one_hot(labels,len(CLASSES)) #precision and recall metrics require one hot labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return dataset.batch(batch_size)\n",
    "    else: \n",
    "        return dataset.shuffle(50000).batch(batch_size)\n",
    "\n",
    "\"\"\"\n",
    "Builds a CNN model using keras and converts to tf.estimator.Estimator\n",
    "  # Arguments\n",
    "      model_dir: string, file path where training files will be written\n",
    "      config: tf.estimator.RunConfig, specifies properties of tf Estimator\n",
    "      filters: int, output dimension of the layers.\n",
    "      kernel_size: int, length of the convolution window.\n",
    "      embedding_dim: int, dimension of the embedding vectors.\n",
    "      dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "      pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "      embedding_path: string , file location of pre-trained embedding (if used)\n",
    "        defaults to None which will cause the model to train embedding from scratch\n",
    "      word_index: dictionary, mapping of vocabulary to integers. used only if\n",
    "        pre-trained embedding is provided\n",
    "\n",
    "    # Returns\n",
    "        A keras model\n",
    "\"\"\"\n",
    "def keras_model(learning_rate):\n",
    "    # Create model instance.\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer\n",
    "    hub_layer = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\", \n",
    "        output_shape=[128], \n",
    "        input_shape=[], \n",
    "        dtype=tf.string\n",
    "    )\n",
    "    model.add(hub_layer)\n",
    "    model.add(Dense(500,activation='relu'))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'train_data_path':'./data/txtcls/train.tsv',\n",
    "    'eval_data_path':'./data/txtcls/eval.tsv',\n",
    "    'batch_size':128,\n",
    "    'learning_rate':.001\n",
    "}\n",
    "\n",
    "# Load Data\n",
    "((train_texts, train_labels), (test_texts, test_labels)) = load_hacker_news_data(\n",
    "    hparams['train_data_path'], hparams['eval_data_path'])\n",
    "\n",
    "model = keras_model(learning_rate=hparams['learning_rate'])\n",
    "\n",
    "train_dataset = input_fn(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    hparams['batch_size'],\n",
    "    mode=tf.estimator.ModeKeys.TRAIN\n",
    ")\n",
    "eval_dataset = input_fn(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    hparams['batch_size'],\n",
    "    mode=tf.estimator.ModeKeys.EVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0815 19:01:44.741532 139943026558720 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "564/564 [==============================] - 8s 14ms/step - loss: 0.5390 - accuracy: 0.7730 - precision: 0.8026 - recall: 0.7330 - val_loss: 0.5125 - val_accuracy: 0.7810 - val_precision: 0.7999 - val_recall: 0.7561\n",
      "Epoch 2/5\n",
      "564/564 [==============================] - 6s 11ms/step - loss: 0.4588 - accuracy: 0.8089 - precision: 0.8289 - recall: 0.7857 - val_loss: 0.4872 - val_accuracy: 0.7930 - val_precision: 0.8098 - val_recall: 0.7736\n",
      "Epoch 3/5\n",
      "564/564 [==============================] - 6s 11ms/step - loss: 0.4187 - accuracy: 0.8285 - precision: 0.8447 - recall: 0.8078 - val_loss: 0.4818 - val_accuracy: 0.7990 - val_precision: 0.8130 - val_recall: 0.7825\n",
      "Epoch 4/5\n",
      "564/564 [==============================] - 6s 11ms/step - loss: 0.3859 - accuracy: 0.8438 - precision: 0.8582 - recall: 0.8267 - val_loss: 0.4854 - val_accuracy: 0.7988 - val_precision: 0.8115 - val_recall: 0.7845\n",
      "Epoch 5/5\n",
      "564/564 [==============================] - 6s 11ms/step - loss: 0.3564 - accuracy: 0.8583 - precision: 0.8702 - recall: 0.8436 - val_loss: 0.4987 - val_accuracy: 0.7969 - val_precision: 0.8079 - val_recall: 0.7845\n",
      "CPU times: user 42.4 s, sys: 6.2 s, total: 48.6 s\n",
      "Wall time: 32.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4618c65f60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(SEED)\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=eval_dataset,\n",
    "    validation_steps=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We get 80% validation accuracy. Not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Level Model with CNN\n",
    "\n",
    "While the above method shines in simplicity, it uses a sentence level embedding which ignores the ordering of words. Might we get better performance if we embedded each word individually then fed them into a sequential model? We test that hypothesis now.\n",
    "\n",
    "The `hub.KerasLayer()` method doesn't support word level embeddings natively, instead it averages the component word embeddings into a single sentence embedding, so to achieve what we want we must do it upfront in the `input_fn()`. In particular we:\n",
    "1. Split each sentence into a list of its component words\n",
    "2. Pad each list to a constant length\n",
    "3. Embed each word into 128 dimension vector representation\n",
    "\n",
    "Note the changes to the `input_fn()` below.\n",
    "\n",
    "Since input function now returns a sequence of word embeddings, so we can process the data using a sequential model. Specifically we'll use a 1D CNN. Note the changes to `keras_model()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "\"\"\"\n",
    "Create tf.estimator compatible input function\n",
    "  # Arguments:\n",
    "      texts: [strings], list of sentences\n",
    "      labels: numpy int vector, integer labels for sentences\n",
    "      tokenizer: tf.python.keras.preprocessing.text.Tokenizer\n",
    "        used to convert sentences to integers\n",
    "      batch_size: int, number of records to use for each train batch\n",
    "      mode: tf.estimator.ModeKeys.TRAIN or tf.estimator.ModeKeys.EVAL \n",
    "  # Returns:\n",
    "      tf.data.dataset, produces feature and label\n",
    "        tensors one batch at a time\n",
    "\"\"\"\n",
    "def input_fn(texts, labels, batch_size, mode):\n",
    "    #precision and recall metrics require one hot labels\n",
    "    labels = tf.one_hot(labels,len(CLASSES)) \n",
    "    #split sentences into lists of words\n",
    "    texts = [sentence.split() for sentence in texts] \n",
    "    # pad to constant length\n",
    "    texts = [(sentence + MAX_SEQUENCE_LENGTH * ['<PAD>'])[:MAX_SEQUENCE_LENGTH] for sentence in texts] \n",
    "    #embed\n",
    "    embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\")\n",
    "    texts = [embed(sentence) for sentence in texts]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return dataset.batch(batch_size)\n",
    "    else: \n",
    "        return dataset.shuffle(50000).batch(batch_size)\n",
    "\n",
    "\"\"\"\n",
    "Builds a CNN model using keras \n",
    "  # Arguments\n",
    "      model_dir: string, file path where training files will be written\n",
    "      config: tf.estimator.RunConfig, specifies properties of tf Estimator\n",
    "      filters: int, output dimension of the layers.\n",
    "      kernel_size: int, length of the convolution window.\n",
    "      embedding_dim: int, dimension of the embedding vectors.\n",
    "      dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "      pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "\n",
    "\n",
    "    # Returns\n",
    "        A tf.estimator.Estimator \n",
    "\"\"\"\n",
    "def keras_model(learning_rate, filters=64, dropout_rate=0.2, kernel_size=3, pool_size=3):\n",
    "    # Create model instance.\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(Dropout(input_shape=(MAX_SEQUENCE_LENGTH,128),rate=dropout_rate))\n",
    "    model.add(Conv1D(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        activation='relu',\n",
    "        bias_initializer='random_uniform',\n",
    "        padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(\n",
    "        filters=filters * 2,\n",
    "        kernel_size=kernel_size,\n",
    "        activation='relu',\n",
    "        bias_initializer='random_uniform',\n",
    "        padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The subsequent cell takes ~ 3 hours on CPU, about ~ 6 minutes on a P100 GPU, and ~ 4 minutes on a V100 GPU**\n",
    "\n",
    "This takes so long because now we are doing a lot of pre-processing in the input function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 12s, sys: 1min 27s, total: 8min 40s\n",
      "Wall time: 6min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = input_fn(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    hparams['batch_size'],\n",
    "    mode=tf.estimator.ModeKeys.TRAIN\n",
    ")\n",
    "eval_dataset = input_fn(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    hparams['batch_size'],\n",
    "    mode=tf.estimator.ModeKeys.EVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "564/564 [==============================] - 10s 18ms/step - loss: 0.6639 - accuracy: 0.7025 - precision_1: 0.7618 - recall_1: 0.6104 - val_loss: 0.5161 - val_accuracy: 0.7870 - val_precision_1: 0.8146 - val_recall_1: 0.7480\n",
      "Epoch 2/5\n",
      "564/564 [==============================] - 7s 13ms/step - loss: 0.4982 - accuracy: 0.7965 - precision_1: 0.8205 - recall_1: 0.7646 - val_loss: 0.4713 - val_accuracy: 0.8063 - val_precision_1: 0.8314 - val_recall_1: 0.7742\n",
      "Epoch 3/5\n",
      "564/564 [==============================] - 8s 13ms/step - loss: 0.4651 - accuracy: 0.8107 - precision_1: 0.8323 - recall_1: 0.7844 - val_loss: 0.4462 - val_accuracy: 0.8185 - val_precision_1: 0.8404 - val_recall_1: 0.7918\n",
      "Epoch 4/5\n",
      "564/564 [==============================] - 8s 13ms/step - loss: 0.4379 - accuracy: 0.8231 - precision_1: 0.8431 - recall_1: 0.7997 - val_loss: 0.4332 - val_accuracy: 0.8234 - val_precision_1: 0.8452 - val_recall_1: 0.7982\n",
      "Epoch 5/5\n",
      "564/564 [==============================] - 8s 14ms/step - loss: 0.4188 - accuracy: 0.8313 - precision_1: 0.8494 - recall_1: 0.8103 - val_loss: 0.4206 - val_accuracy: 0.8288 - val_precision_1: 0.8476 - val_recall_1: 0.8048\n",
      "CPU times: user 1min 1s, sys: 12.9 s, total: 1min 14s\n",
      "Wall time: 57.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f45d42afe80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = keras_model(learning_rate=hparams['learning_rate'])\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=eval_dataset,\n",
    "    validation_steps=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy improved to 83%! Looks like paying attention to word order does help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('headline_classification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy trained model \n",
    "\n",
    "See [deploy.ipynb](deploy.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
