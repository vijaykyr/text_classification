{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Classification using TensorFlow/Keras on Cloud ML Engine </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating datasets for Machine Learning using BigQuery\n",
    "<li> Creating a text classification model using the Estimator API with a Keras model\n",
    "<li> Training on Cloud ML Engine\n",
    "<li> Deploying the model\n",
    "<li> Predicting with model\n",
    "<li> Rerun with pre-trained embedding\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'vijays-sandbox-ml'\n",
    "PROJECT = 'vijays-sandbox'\n",
    "REGION = 'us-central1'\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__) # tf 2.0 nightly\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "CLASSES = {'github': 0, 'nytimes': 1, 'techcrunch': 2}  # label-to-int mapping\n",
    "TOP_K = 20000  # Limit on the number vocabulary size used for tokenization\n",
    "MAX_SEQUENCE_LENGTH = 50  # Sentences will be truncated/padded to this length\n",
    "\n",
    "\"\"\"\n",
    "Helper function to download data from Google Cloud Storage\n",
    "  # Arguments:\n",
    "      source: string, the GCS URL to download from (e.g. 'gs://bucket/file.csv')\n",
    "      destination: string, the filename to save as on local disk. MUST be filename\n",
    "      ONLY, doesn't support folders. (e.g. 'file.csv', NOT 'folder/file.csv')\n",
    "  # Returns: nothing, downloads file to local disk\n",
    "\"\"\"\n",
    "def download_from_gcs(source, destination):\n",
    "    search = re.search('gs://(.*?)/(.*)', source)\n",
    "    bucket_name = search.group(1)\n",
    "    blob_name = search.group(2)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    bucket.blob(blob_name).download_to_filename(destination)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parses raw tsv containing hacker news headlines and returns (sentence, integer label) pairs\n",
    "  # Arguments:\n",
    "      train_data_path: string, path to tsv containing training data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "      eval_data_path: string, path to tsv containing eval data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "  # Returns:\n",
    "      ((train_sentences, train_labels), (test_sentences, test_labels)):  sentences\n",
    "        are lists of strings, labels are numpy integer arrays\n",
    "\"\"\"\n",
    "def load_hacker_news_data(train_data_path, eval_data_path):\n",
    "    if train_data_path.startswith('gs://'):\n",
    "        download_from_gcs(train_data_path, destination='train.csv')\n",
    "        train_data_path = 'train.csv'\n",
    "    if eval_data_path.startswith('gs://'):\n",
    "        download_from_gcs(eval_data_path, destination='eval.csv')\n",
    "        eval_data_path = 'eval.csv'\n",
    "\n",
    "    # Parse CSV using pandas\n",
    "    column_names = ('label', 'text')\n",
    "    df_train = pd.read_csv(train_data_path, names=column_names, sep='\\t')\n",
    "    df_eval = pd.read_csv(eval_data_path, names=column_names, sep='\\t')\n",
    "\n",
    "    return ((list(df_train['text']), np.array(df_train['label'].map(CLASSES))),\n",
    "            (list(df_eval['text']), np.array(df_eval['label'].map(CLASSES))))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create tf.estimator compatible input function\n",
    "  # Arguments:\n",
    "      texts: [strings], list of sentences\n",
    "      labels: numpy int vector, integer labels for sentences\n",
    "      tokenizer: tf.python.keras.preprocessing.text.Tokenizer\n",
    "        used to convert sentences to integers\n",
    "      batch_size: int, number of records to use for each train batch\n",
    "      mode: tf.estimator.ModeKeys.TRAIN or tf.estimator.ModeKeys.EVAL \n",
    "  # Returns:\n",
    "      tf.data.dataset, produces feature and label\n",
    "        tensors one batch at a time\n",
    "\"\"\"\n",
    "def input_fn(texts, labels, batch_size, mode):\n",
    "    def _embed(sentence,label):\n",
    "        #embeddings = embed(sentence)\n",
    "        #return embeddings,label\n",
    "        return sentence,label\n",
    "    \n",
    "    #precision and recall metrics require one hot labels\n",
    "    labels = tf.one_hot(labels,len(CLASSES)) \n",
    "    #split sentences into lists of words\n",
    "    texts = [sentence.split() for sentence in texts] \n",
    "    # pad to constant length\n",
    "    texts = [(sentence + MAX_SEQUENCE_LENGTH * ['<PAD>'])[:MAX_SEQUENCE_LENGTH] for sentence in texts] \n",
    "    #embed\n",
    "    embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\")\n",
    "    texts = [embed(sentence) for sentence in texts]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return dataset.batch(batch_size)\n",
    "    else: \n",
    "        return dataset.shuffle(50000).batch(batch_size)\n",
    "\n",
    "\"\"\"\n",
    "Builds a CNN model using keras and converts to tf.estimator.Estimator\n",
    "  # Arguments\n",
    "      model_dir: string, file path where training files will be written\n",
    "      config: tf.estimator.RunConfig, specifies properties of tf Estimator\n",
    "      filters: int, output dimension of the layers.\n",
    "      kernel_size: int, length of the convolution window.\n",
    "      embedding_dim: int, dimension of the embedding vectors.\n",
    "      dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "      pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "      embedding_path: string , file location of pre-trained embedding (if used)\n",
    "        defaults to None which will cause the model to train embedding from scratch\n",
    "      word_index: dictionary, mapping of vocabulary to integers. used only if\n",
    "        pre-trained embedding is provided\n",
    "\n",
    "    # Returns\n",
    "        A tf.estimator.Estimator \n",
    "\"\"\"\n",
    "def keras_estimator(model_dir,\n",
    "                    config,\n",
    "                    learning_rate,\n",
    "                    filters=64,\n",
    "                    dropout_rate=0.2,\n",
    "                    embedding_dim=200,\n",
    "                    kernel_size=3,\n",
    "                    pool_size=3,\n",
    "                    embedding_path=None,\n",
    "                    word_index=None):\n",
    "    # Create model instance.\n",
    "    model = models.Sequential()\n",
    "\n",
    "    #model.add(tf.keras.layers.InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,128)))\n",
    "    model.add(Dropout(input_shape=(MAX_SEQUENCE_LENGTH,128),rate=dropout_rate))\n",
    "    model.add(Conv1D(\n",
    "        #input_shape=(MAX_SEQUENCE_LENGTH,128),\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        activation='relu',\n",
    "        bias_initializer='random_uniform',\n",
    "        padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(\n",
    "        filters=filters * 2,\n",
    "        kernel_size=kernel_size,\n",
    "        activation='relu',\n",
    "        bias_initializer='random_uniform',\n",
    "        padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Defines the features to be passed to the model during inference \n",
    "  Expects already tokenized and padded representation of sentences\n",
    "  # Arguments: none\n",
    "  # Returns: tf.estimator.export.ServingInputReceiver\n",
    "\"\"\"\n",
    "def serving_input_fn():\n",
    "    feature_placeholder = tf.compat.v1.placeholder(tf.string, [None])\n",
    "    features = feature_placeholder  # pass as-is\n",
    "    return tf.estimator.export.TensorServingInputReceiver(features, feature_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {'train_data_path':'./data/txtcls/train.tsv',\n",
    "           'eval_data_path':'./data/txtcls/eval.tsv'}\n",
    "\n",
    "# Load Data\n",
    "((train_texts, train_labels), (test_texts, test_labels)) = load_hacker_news_data(\n",
    "    hparams['train_data_path'], hparams['eval_data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=48280, shape=(3, 50, 128), dtype=float32, numpy=\n",
      "array([[[ 0.05921581,  0.2607566 , -0.01803753, ..., -0.09629437,\n",
      "          0.1261976 , -0.10059416],\n",
      "        [-0.05234646, -0.04587635,  0.09989155, ...,  0.10251415,\n",
      "         -0.04885277, -0.02132634],\n",
      "        [-0.23469844,  0.13233985,  0.15608735, ...,  0.13845624,\n",
      "         -0.14491926,  0.08018108],\n",
      "        ...,\n",
      "        [-0.24159245,  0.06395964, -0.07213151, ...,  0.24148819,\n",
      "         -0.09287039,  0.11624853],\n",
      "        [-0.24159245,  0.06395964, -0.07213151, ...,  0.24148819,\n",
      "         -0.09287039,  0.11624853],\n",
      "        [-0.24159245,  0.06395964, -0.07213151, ...,  0.24148819,\n",
      "         -0.09287039,  0.11624853]],\n",
      "\n",
      "       [[-0.08414377,  0.00414404,  0.06067044, ...,  0.04296787,\n",
      "         -0.09396288, -0.15785182],\n",
      "        [-0.04778689,  0.10285304,  0.04732528, ..., -0.10652717,\n",
      "         -0.01119212, -0.0911274 ],\n",
      "        [ 0.23039751, -0.12019835, -0.03054287, ...,  0.07649281,\n",
      "         -0.08489004, -0.01898622],\n",
      "        ...,\n",
      "        [-0.24159245,  0.06395964, -0.07213151, ...,  0.24148819,\n",
      "         -0.09287039,  0.11624853],\n",
      "        [-0.24159245,  0.06395964, -0.07213151, ...,  0.24148819,\n",
      "         -0.09287039,  0.11624853],\n",
      "        [-0.24159245,  0.06395964, -0.07213151, ...,  0.24148819,\n",
      "         -0.09287039,  0.11624853]],\n",
      "\n",
      "       [[-0.15692437,  0.03606329,  0.11764601, ...,  0.13537633,\n",
      "          0.0594439 , -0.06769142],\n",
      "        [-0.07597167,  0.03274958, -0.11897359, ...,  0.14735857,\n",
      "         -0.00863433, -0.2134745 ],\n",
      "        [ 0.23039751, -0.12019835, -0.03054287, ...,  0.07649281,\n",
      "         -0.08489004, -0.01898622],\n",
      "        ...,\n",
      "        [-0.24159245,  0.06395964, -0.07213151, ...,  0.24148819,\n",
      "         -0.09287039,  0.11624853],\n",
      "        [-0.24159245,  0.06395964, -0.07213151, ...,  0.24148819,\n",
      "         -0.09287039,  0.11624853],\n",
      "        [-0.24159245,  0.06395964, -0.07213151, ...,  0.24148819,\n",
      "         -0.09287039,  0.11624853]]], dtype=float32)>, <tf.Tensor: id=48281, shape=(3, 3), dtype=float32, numpy=\n",
      "array([[1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.]], dtype=float32)>)\n",
      "CPU times: user 1min 17s, sys: 12.6 s, total: 1min 30s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = input_fn(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    3,\n",
    "    mode=tf.estimator.ModeKeys.EVAL\n",
    ")\n",
    "for batch in dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['show hn  scrwl   shorthand code reading and writing language',\n",
       " 'geoip module on nodejs now is a c   addon',\n",
       " 'vilfredo pareto on software product management',\n",
       " ' walking around the earth   street view virtual walking on google earth',\n",
       " 'projector - easy mockup presentation engine']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 9s, sys: 45.2 s, total: 5min 54s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hparams = {'batch_size':128}\n",
    "\n",
    "train_dataset = input_fn(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    hparams['batch_size'],\n",
    "    mode=tf.estimator.ModeKeys.TRAIN\n",
    ")\n",
    "eval_dataset = input_fn(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    hparams['batch_size'],\n",
    "    mode=tf.estimator.ModeKeys.EVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_2 (Dropout)          (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 50, 64)            24640     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16, 128)           24704     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 49,731\n",
      "Trainable params: 49,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hparams = {'learning_rate':.001}\n",
    "\n",
    "model = keras_estimator(\n",
    "    model_dir='output_dir',\n",
    "    config=None,\n",
    "    learning_rate=hparams['learning_rate'],\n",
    "    embedding_path=None,\n",
    "    word_index=None\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564/564 [==============================] - 9s 16ms/step - loss: 0.6671 - accuracy: 0.7038 - precision_1: 0.7644 - recall_1: 0.6086 - val_loss: 0.5010 - val_accuracy: 0.7940 - val_precision_1: 0.8230 - val_recall_1: 0.7558\n",
      "CPU times: user 12.9 s, sys: 2.87 s, total: 15.8 s\n",
      "Wall time: 12.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fead47f5b70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(SEED)\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1,\n",
    "    validation_data=eval_dataset,\n",
    "    validation_steps=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('headline_classification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving \n",
    "\n",
    "We need to do custom model serving. See notebook LINK to deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
