{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Classification using TensorFlow/Keras on Cloud ML Engine </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating datasets for Machine Learning using BigQuery\n",
    "<li> Creating a text classification model using the Estimator API with a Keras model\n",
    "<li> Training on Cloud ML Engine\n",
    "<li> Deploying the model\n",
    "<li> Predicting with model\n",
    "<li> Rerun with pre-trained embedding\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'vijays-sandbox-ml'\n",
    "PROJECT = 'vijays-sandbox'\n",
    "REGION = 'us-central1'\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__) # tf 2.0 nightly\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'glorot_uniform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbias_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "1D convolution layer (e.g. temporal convolution).\n",
       "\n",
       "This layer creates a convolution kernel that is convolved\n",
       "with the layer input over a single spatial (or temporal) dimension\n",
       "to produce a tensor of outputs.\n",
       "If `use_bias` is True, a bias vector is created and added to the outputs.\n",
       "Finally, if `activation` is not `None`,\n",
       "it is applied to the outputs as well.\n",
       "\n",
       "When using this layer as the first layer in a model,\n",
       "provide an `input_shape` argument\n",
       "(tuple of integers or `None`, e.g.\n",
       "`(10, 128)` for sequences of 10 vectors of 128-dimensional vectors,\n",
       "or `(None, 128)` for variable-length sequences of 128-dimensional vectors.\n",
       "\n",
       "Arguments:\n",
       "  filters: Integer, the dimensionality of the output space\n",
       "    (i.e. the number of output filters in the convolution).\n",
       "  kernel_size: An integer or tuple/list of a single integer,\n",
       "    specifying the length of the 1D convolution window.\n",
       "  strides: An integer or tuple/list of a single integer,\n",
       "    specifying the stride length of the convolution.\n",
       "    Specifying any stride value != 1 is incompatible with specifying\n",
       "    any `dilation_rate` value != 1.\n",
       "  padding: One of `\"valid\"`, `\"causal\"` or `\"same\"` (case-insensitive).\n",
       "    `\"causal\"` results in causal (dilated) convolutions, e.g. output[t]\n",
       "    does not depend on input[t+1:]. Useful when modeling temporal data\n",
       "    where the model should not violate the temporal order.\n",
       "    See [WaveNet: A Generative Model for Raw Audio, section\n",
       "      2.1](https://arxiv.org/abs/1609.03499).\n",
       "  data_format: A string,\n",
       "    one of `channels_last` (default) or `channels_first`.\n",
       "  dilation_rate: an integer or tuple/list of a single integer, specifying\n",
       "    the dilation rate to use for dilated convolution.\n",
       "    Currently, specifying any `dilation_rate` value != 1 is\n",
       "    incompatible with specifying any `strides` value != 1.\n",
       "  activation: Activation function to use.\n",
       "    If you don't specify anything, no activation is applied\n",
       "    (ie. \"linear\" activation: `a(x) = x`).\n",
       "  use_bias: Boolean, whether the layer uses a bias vector.\n",
       "  kernel_initializer: Initializer for the `kernel` weights matrix.\n",
       "  bias_initializer: Initializer for the bias vector.\n",
       "  kernel_regularizer: Regularizer function applied to\n",
       "    the `kernel` weights matrix.\n",
       "  bias_regularizer: Regularizer function applied to the bias vector.\n",
       "  activity_regularizer: Regularizer function applied to\n",
       "    the output of the layer (its \"activation\")..\n",
       "  kernel_constraint: Constraint function applied to the kernel matrix.\n",
       "  bias_constraint: Constraint function applied to the bias vector.\n",
       "\n",
       "Input shape:\n",
       "  3D tensor with shape: `(batch_size, steps, input_dim)`\n",
       "\n",
       "Output shape:\n",
       "  3D tensor with shape: `(batch_size, new_steps, filters)`\n",
       "    `steps` value might have changed due to padding or strides.\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/convolutional.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Conv1D\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.keras.layers.Conv1D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "CLASSES = {'github': 0, 'nytimes': 1, 'techcrunch': 2}  # label-to-int mapping\n",
    "TOP_K = 20000  # Limit on the number vocabulary size used for tokenization\n",
    "MAX_SEQUENCE_LENGTH = 50  # Sentences will be truncated/padded to this length\n",
    "\n",
    "\"\"\"\n",
    "Helper function to download data from Google Cloud Storage\n",
    "  # Arguments:\n",
    "      source: string, the GCS URL to download from (e.g. 'gs://bucket/file.csv')\n",
    "      destination: string, the filename to save as on local disk. MUST be filename\n",
    "      ONLY, doesn't support folders. (e.g. 'file.csv', NOT 'folder/file.csv')\n",
    "  # Returns: nothing, downloads file to local disk\n",
    "\"\"\"\n",
    "def download_from_gcs(source, destination):\n",
    "    search = re.search('gs://(.*?)/(.*)', source)\n",
    "    bucket_name = search.group(1)\n",
    "    blob_name = search.group(2)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    bucket.blob(blob_name).download_to_filename(destination)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parses raw tsv containing hacker news headlines and returns (sentence, integer label) pairs\n",
    "  # Arguments:\n",
    "      train_data_path: string, path to tsv containing training data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "      eval_data_path: string, path to tsv containing eval data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "  # Returns:\n",
    "      ((train_sentences, train_labels), (test_sentences, test_labels)):  sentences\n",
    "        are lists of strings, labels are numpy integer arrays\n",
    "\"\"\"\n",
    "def load_hacker_news_data(train_data_path, eval_data_path):\n",
    "    if train_data_path.startswith('gs://'):\n",
    "        download_from_gcs(train_data_path, destination='train.csv')\n",
    "        train_data_path = 'train.csv'\n",
    "    if eval_data_path.startswith('gs://'):\n",
    "        download_from_gcs(eval_data_path, destination='eval.csv')\n",
    "        eval_data_path = 'eval.csv'\n",
    "\n",
    "    # Parse CSV using pandas\n",
    "    column_names = ('label', 'text')\n",
    "    df_train = pd.read_csv(train_data_path, names=column_names, sep='\\t')\n",
    "    df_eval = pd.read_csv(eval_data_path, names=column_names, sep='\\t')\n",
    "\n",
    "    return ((list(df_train['text']), np.array(df_train['label'].map(CLASSES))),\n",
    "            (list(df_eval['text']), np.array(df_eval['label'].map(CLASSES))))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create tf.estimator compatible input function\n",
    "  # Arguments:\n",
    "      texts: [strings], list of sentences\n",
    "      labels: numpy int vector, integer labels for sentences\n",
    "      tokenizer: tf.python.keras.preprocessing.text.Tokenizer\n",
    "        used to convert sentences to integers\n",
    "      batch_size: int, number of records to use for each train batch\n",
    "      mode: tf.estimator.ModeKeys.TRAIN or tf.estimator.ModeKeys.EVAL \n",
    "  # Returns:\n",
    "      tf.data.dataset, produces feature and label\n",
    "        tensors one batch at a time\n",
    "\"\"\"\n",
    "def input_fn(texts, labels, batch_size, mode):\n",
    "    def _embed(sentence,label):\n",
    "        #embeddings = embed(sentence)\n",
    "        #return embeddings,label\n",
    "        return sentence,label\n",
    "    \n",
    "    # Transform text to sequence of integers\n",
    "    labels = tf.one_hot(labels,len(CLASSES)) #precision and recall metrics require one hot labels\n",
    "    \n",
    "    texts = [sentence.split() for sentence in texts]\n",
    "    #texts = [(MAX_SEQUENCE_LENGTH * ['<PAD>'] + sentence)[-MAX_SEQUENCE_LENGTH:] for sentence in texts]\n",
    "    texts = [(sentence + MAX_SEQUENCE_LENGTH * ['<PAD>'])[:MAX_SEQUENCE_LENGTH] for sentence in texts]\n",
    "    embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\")\n",
    "    texts = [embed(sentence) for sentence in texts]\n",
    "    #1. Recieve string tensor : string ()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    \n",
    "    \n",
    "    #dataset = dataset.map(_embed) # will need to change to flat_map\n",
    "    #2. Tokenize: string (?,)\n",
    "    #3. Pad (later try replacing with ragged tensor): string(max_seq_len,)\n",
    "    #4. Embed w/ tf hub:  float (max_seq_len,embed_dim\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return dataset.batch(batch_size)\n",
    "    else: \n",
    "        return dataset.shuffle(50000).batch(batch_size)\n",
    "\n",
    "\"\"\"\n",
    "Builds a CNN model using keras and converts to tf.estimator.Estimator\n",
    "  # Arguments\n",
    "      model_dir: string, file path where training files will be written\n",
    "      config: tf.estimator.RunConfig, specifies properties of tf Estimator\n",
    "      filters: int, output dimension of the layers.\n",
    "      kernel_size: int, length of the convolution window.\n",
    "      embedding_dim: int, dimension of the embedding vectors.\n",
    "      dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "      pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "      embedding_path: string , file location of pre-trained embedding (if used)\n",
    "        defaults to None which will cause the model to train embedding from scratch\n",
    "      word_index: dictionary, mapping of vocabulary to integers. used only if\n",
    "        pre-trained embedding is provided\n",
    "\n",
    "    # Returns\n",
    "        A tf.estimator.Estimator \n",
    "\"\"\"\n",
    "def keras_estimator(model_dir,\n",
    "                    config,\n",
    "                    learning_rate,\n",
    "                    filters=64,\n",
    "                    dropout_rate=0.2,\n",
    "                    embedding_dim=200,\n",
    "                    kernel_size=3,\n",
    "                    pool_size=3,\n",
    "                    embedding_path=None,\n",
    "                    word_index=None):\n",
    "    # Create model instance.\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer\n",
    "    #hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\", output_shape=[128], \n",
    "    #                   input_shape=[], dtype=tf.string)\n",
    "    #model.add(hub_layer)\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,128)))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Conv1D(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        activation='relu',\n",
    "        bias_initializer='random_uniform',\n",
    "        padding='same'))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Defines the features to be passed to the model during inference \n",
    "  Expects already tokenized and padded representation of sentences\n",
    "  # Arguments: none\n",
    "  # Returns: tf.estimator.export.ServingInputReceiver\n",
    "\"\"\"\n",
    "def serving_input_fn():\n",
    "    feature_placeholder = tf.compat.v1.placeholder(tf.string, [None])\n",
    "    features = feature_placeholder  # pass as-is\n",
    "    return tf.estimator.export.TensorServingInputReceiver(features, feature_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=872737, shape=(3, 50), dtype=string, numpy=\n",
      "array([[b'show', b'hn', b'scrwl', b'shorthand', b'code', b'reading',\n",
      "        b'and', b'writing', b'language', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>'],\n",
      "       [b'geoip', b'module', b'on', b'nodejs', b'now', b'is', b'a', b'c',\n",
      "        b'addon', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>'],\n",
      "       [b'vilfredo', b'pareto', b'on', b'software', b'product',\n",
      "        b'management', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
      "        b'<PAD>', b'<PAD>', b'<PAD>']], dtype=object)>, <tf.Tensor: id=872738, shape=(3, 3), dtype=float32, numpy=\n",
      "array([[1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.]], dtype=float32)>)\n",
      "CPU times: user 4 s, sys: 1.06 s, total: 5.05 s\n",
      "Wall time: 7.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = input_fn(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    3,\n",
    "    mode=tf.estimator.ModeKeys.EVAL\n",
    ")\n",
    "for batch in dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 9s, sys: 48.9 s, total: 5min 58s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hparams = {'train_data_path':'./data/txtcls/train.tsv',\n",
    "           'eval_data_path':'./data/txtcls/eval.tsv',\n",
    "           'batch_size':128}\n",
    "# Load Data\n",
    "((train_texts, train_labels), (test_texts, test_labels)) = load_hacker_news_data(\n",
    "    hparams['train_data_path'], hparams['eval_data_path'])\n",
    "\n",
    "\n",
    "train_dataset = input_fn(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    hparams['batch_size'],\n",
    "    mode=tf.estimator.ModeKeys.TRAIN\n",
    ")\n",
    "eval_dataset = input_fn(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    hparams['batch_size'],\n",
    "    mode=tf.estimator.ModeKeys.EVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_11 (Dropout)         (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 50, 64)            24640     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16, 128)           24704     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_6 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 49,731\n",
      "Trainable params: 49,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hparams = {'learning_rate':.001,\n",
    "           'num_epochs':3}\n",
    "model = keras_estimator(\n",
    "    model_dir='output_dir',\n",
    "    config=None,\n",
    "    learning_rate=hparams['learning_rate'],\n",
    "    embedding_path=None,\n",
    "    word_index=None\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "564/564 [==============================] - 9s 15ms/step - loss: 0.6729 - accuracy: 0.6992 - precision_7: 0.7601 - recall_7: 0.6001 - val_loss: 0.5136 - val_accuracy: 0.7909 - val_precision_7: 0.8195 - val_recall_7: 0.7487\n",
      "Epoch 2/5\n",
      "564/564 [==============================] - 7s 13ms/step - loss: 0.5014 - accuracy: 0.7940 - precision_7: 0.8183 - recall_7: 0.7615 - val_loss: 0.4689 - val_accuracy: 0.8079 - val_precision_7: 0.8331 - val_recall_7: 0.7769\n",
      "Epoch 3/5\n",
      "564/564 [==============================] - 7s 12ms/step - loss: 0.4650 - accuracy: 0.8100 - precision_7: 0.8317 - recall_7: 0.7833 - val_loss: 0.4437 - val_accuracy: 0.8186 - val_precision_7: 0.8417 - val_recall_7: 0.7913\n",
      "Epoch 4/5\n",
      "564/564 [==============================] - 7s 12ms/step - loss: 0.4389 - accuracy: 0.8226 - precision_7: 0.8429 - recall_7: 0.7983 - val_loss: 0.4270 - val_accuracy: 0.8251 - val_precision_7: 0.8475 - val_recall_7: 0.8010\n",
      "Epoch 5/5\n",
      "564/564 [==============================] - 7s 12ms/step - loss: 0.4176 - accuracy: 0.8299 - precision_7: 0.8487 - recall_7: 0.8085 - val_loss: 0.4177 - val_accuracy: 0.8295 - val_precision_7: 0.8486 - val_recall_7: 0.8066\n",
      "CPU times: user 58.2 s, sys: 11.1 s, total: 1min 9s\n",
      "Wall time: 55.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f70610f9710>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(SEED)\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=eval_dataset,\n",
    "    validation_steps=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
